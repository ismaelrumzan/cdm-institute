---
title: "5.3 Retrieving Information Tool"
description: "Implement a tool to search and retrieve relevant information from your knowledge base using semantic similarity."
---

import { AIPromptReflection } from "/snippets/ai-prompt-reflection.jsx";

## The Missing Piece: Information Retrieval

The model can now add and embed arbitrary information to your knowledge base. However, it still isn't able to query it. Let's create a new tool to allow the model to answer questions by finding relevant information in your knowledge base.

<Info>
  To find similar content, you will need to embed the user's query, search the
  database for semantic similarities, then pass those items to the model as
  context alongside the query.
</Info>

---

## Updating Embedding Logic

First, let's update your embedding logic file (`lib/ai/embedding.ts`) to add functions for finding relevant content:

```typescript lib/ai/embedding.ts lines highlight="1,3-5,27-34, 36-49"
import { embed, embedMany } from "ai";
import { openai } from "@ai-sdk/openai";
import { db } from "../db";
import { cosineDistance, desc, gt, sql } from "drizzle-orm";
import { embeddings } from "../db/schema/embeddings";

const embeddingModel = openai.embedding("text-embedding-ada-002");

const generateChunks = (input: string): string[] => {
  return input
    .trim()
    .split(".")
    .filter((i) => i !== "");
};

export const generateEmbeddings = async (
  value: string
): Promise<Array<{ embedding: number[]; content: string }>> => {
  const chunks = generateChunks(value);
  const { embeddings } = await embedMany({
    model: embeddingModel,
    values: chunks,
  });
  return embeddings.map((e, i) => ({ content: chunks[i], embedding: e }));
};

export const generateEmbedding = async (value: string): Promise<number[]> => {
  const input = value.replaceAll("\\n", " ");
  const { embedding } = await embed({
    model: embeddingModel,
    value: input,
  });
  return embedding;
};

export const findRelevantContent = async (userQuery: string) => {
  const userQueryEmbedded = await generateEmbedding(userQuery);
  const similarity = sql<number>`1 - (${cosineDistance(
    embeddings.embedding,
    userQueryEmbedded
  )})`;
  const similarGuides = await db
    .select({ name: embeddings.content, similarity })
    .from(embeddings)
    .where(gt(similarity, 0.5))
    .orderBy((t) => desc(t.similarity))
    .limit(4);
  return similarGuides;
};
```

### New Functions Explained

<Steps>
  <Step title="generateEmbedding">
    Generates a single embedding from an input string for query purposes.
  </Step>
  <Step title="findRelevantContent">
    Embeds the user's query, searches the database for similar items using
    cosine similarity, then returns relevant items.
  </Step>
  <Step title="Similarity Threshold">
    Only returns results with similarity greater than 0.5 to ensure relevance.
  </Step>
  <Step title="Result Limiting">
    Limits results to 4 items to avoid overwhelming the model with context.
  </Step>
</Steps>

---

## Adding the Information Retrieval Tool

Now, go back to your route handler (`app/api/chat/route.ts`) and add a new tool called `getInformation`:

```typescript app/api/chat/route.ts lines highlight="11,37-43"
import { createResource } from "@/lib/actions/resources";
import { openai } from "@ai-sdk/openai";
import {
  convertToModelMessages,
  streamText,
  tool,
  UIMessage,
  stepCountIs,
} from "ai";
import { z } from "zod";
import { findRelevantContent } from "@/lib/ai/embedding";

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    messages: convertToModelMessages(messages),
    stopWhen: stepCountIs(5),
    system: `You are a helpful assistant. Check your knowledge base before answering any questions.
    Only respond to questions using information from tool calls.
    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,
    tools: {
      addResource: tool({
        description: `add a resource to your knowledge base.
          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,
        inputSchema: z.object({
          content: z
            .string()
            .describe("the content or resource to add to your knowledge base"),
        }),
        execute: async ({ content }) => createResource({ content }),
      }),
      getInformation: tool({
        description: `get information from your knowledge base to answer questions.`,
        inputSchema: z.object({
          question: z.string().describe("the users question"),
        }),
        execute: async ({ question }) => findRelevantContent(question),
      }),
    },
  });

  return result.toUIMessageStreamResponse();
}
```

---

## Understanding Semantic Search

The `findRelevantContent` function uses vector similarity to find the most relevant information:

<Columns cols={2}>
  <Card title="Query Embedding" icon="search">
    **Vector Conversion**: The user's question is converted to a vector
    representation.
  </Card>
  <Card title="Similarity Calculation" icon="zap">
    **Cosine Distance**: Measures similarity between query and stored
    embeddings.
  </Card>
</Columns>
<Columns cols={2}>
  <Card title="Threshold Filtering" icon="filter">
    **Relevance Filter**: Only returns results above 0.5 similarity threshold.
  </Card>
  <Card title="Ranked Results" icon="list">
    **Best Matches**: Results are ordered by similarity score (highest first).
  </Card>
</Columns>

---

## Testing the Complete RAG Flow

<Steps>
  <Step title="Refresh the Page">
    Head back to the browser, refresh the page to ensure the new tool is loaded.
  </Step>
  <Step title="Ask a Question">
    Ask for your favorite food (or any information you previously added to the
    knowledge base).
  </Step>
  <Step title="Observe Tool Calls">
    You should see the model call the `getInformation` tool, then use the
    relevant information to formulate a response.
  </Step>
  <Step title="Verify Response Quality">
    The model should now provide informative answers based on your stored
    knowledge.
  </Step>
</Steps>

<Info>
  With both tools implemented, you now have a complete RAG system! The AI can
  both store new information and retrieve relevant content to answer questions.
</Info>

---

## How the Complete Flow Works

Here's the complete RAG flow in action:

1. **User Input**: User asks a question or provides information
2. **AI Analysis**: Model determines whether to add information or retrieve it
3. **Tool Selection**: Model chooses between `addResource` or `getInformation`
4. **Tool Execution**: Selected tool performs its function
5. **Result Processing**: Tool results are sent back to the model
6. **Response Generation**: Model generates a response using the tool results
7. **User Output**: Final response is streamed to the user

<Columns cols={2}>
  <Card title="Information Addition" icon="plus">
    <ul>
      <li>**When**: User provides new information</li>{" "}
      <li>**Tool**: `addResource`</li>{" "}
      <li>**Result**: Information stored and embedded</li>
    </ul>
  </Card>
  <Card title="Information Retrieval" icon="search">
    <ul>
      <li>**When**: User asks a question</li>{" "}
      <li>**Tool**: `getInformation`</li>{" "}
      <li>**Result**: Relevant content found and used</li>
    </ul>
  </Card>
</Columns>

---

## Testing Different Scenarios

<Steps>
  <Step title="Test Information Addition">
    Tell the model various pieces of information to build up your knowledge
    base.
  </Step>
  <Step title="Test Information Retrieval">
    Ask questions about the information you've added to see how well the
    retrieval works.
  </Step>
  <Step title="Test Edge Cases">
    Try asking questions about topics you haven't covered to see the "Sorry, I
    don't know" response.
  </Step>
  <Step title="Test Similarity Matching">
    Use different phrasings to ask the same question and see how well semantic
    search works.
  </Step>
</Steps>

---

## Understanding Similarity Scores

The similarity threshold of 0.5 means:

- **0.5-1.0**: High similarity, very relevant content
- **0.7-0.9**: Excellent matches, highly relevant
- **0.5-0.7**: Good matches, relevant content
- **Below 0.5**: Too dissimilar, filtered out

<Info>
  You can adjust the similarity threshold based on your needs. Lower values
  return more results but may be less relevant, while higher values ensure only
  the most relevant content is returned.
</Info>

---

## Extension tasks

<AIPromptReflection
  cardTitle="Semantic Search Optimization"
  question="How would you optimize the semantic search algorithm for better performance and accuracy? What factors affect the quality of vector similarity search?"
  chatgptButtonText="Ask ChatGPT"
  claudeButtonText="Ask Claude"
/>

<AIPromptReflection
  cardTitle="RAG System Design"
  question="What are the key considerations when designing a production RAG system? How do you balance accuracy, performance, and user experience?"
  chatgptButtonText="Ask ChatGPT"
  claudeButtonText="Ask Claude"
/>

```

```
