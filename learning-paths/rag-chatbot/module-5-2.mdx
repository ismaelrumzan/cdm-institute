---
title: "5.2 Multi-Step Tool Calls"
description: "Configure multi-step tool calls to provide better user experience and confirmation of actions."
---

import { AIPromptReflection } from "/snippets/ai-prompt-reflection.jsx";

## Improving UX with Multi-Step Calls

It would be nice if the model could summarize the action too. However, technically, once the model calls a tool, it has completed its generation as it 'generated' a tool call. How could you achieve this desired behavior?

<Info>
  The AI SDK has a feature called `stopWhen` which allows stopping conditions
  when the model generates a tool call. If those stopping conditions haven't
  been hit, the AI SDK will automatically send tool call results back to the
  model!
</Info>

---

## Configuring Multi-Step Processing

Open your route handler (`app/api/chat/route.ts`) and add the following key to the `streamText` configuration object:

```typescript app/api/chat/route.ts lines highlight="8,24"
import { createResource } from "@/lib/actions/resources";
import { openai } from "@ai-sdk/openai";
import {
  convertToModelMessages,
  streamText,
  tool,
  UIMessage,
  stepCountIs,
} from "ai";
import { z } from "zod";

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    system: `You are a helpful assistant. Check your knowledge base before answering any questions.
    Only respond to questions using information from tool calls.
    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,
    messages: convertToModelMessages(messages),
    stopWhen: stepCountIs(5),
    tools: {
      addResource: tool({
        description: `add a resource to your knowledge base.
          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,
        inputSchema: z.object({
          content: z
            .string()
            .describe("the content or resource to add to the knowledge base"),
        }),
        execute: async ({ content }) => createResource({ content }),
      }),
    },
  });

  return result.toUIMessageStreamResponse();
}
```

### How `stopWhen` Works

<Steps>
  <Step title="Tool Call Generation">
    The model generates a tool call and completes its initial generation.
  </Step>
  <Step title="Tool Execution">
    The AI SDK automatically executes the tool with the provided parameters.
  </Step>
  <Step title="Result Processing">
    Tool results are sent back to the model for additional processing.
  </Step>
  <Step title="Continued Generation">
    The model can now provide additional context, confirmation, or summary.
  </Step>
</Steps>

---

## Understanding Step Count

The `stepCountIs(5)` configuration means the model can continue generating for up to 5 steps after the initial tool call. This allows for:

<Columns cols={2}>
  <Card title="Action Confirmation" icon="check-circle">
    **User Feedback**: The model can confirm what action was taken and provide
    context.
  </Card>
  <Card title="Additional Information" icon="info">
    **Enhanced Responses**: The model can add relevant details or suggestions.
  </Card>
</Columns>
<Columns cols={2}>
  <Card title="Error Handling" icon="alert-triangle">
    **Graceful Recovery**: The model can handle tool execution errors and
    provide alternatives.
  </Card>
  <Card title="Context Building" icon="layers">
    **Knowledge Expansion**: The model can build upon the tool results with
    additional insights.
  </Card>
</Columns>

---

## Testing Multi-Step Functionality

<Steps>
  <Step title="Test the Enhanced Behavior">
    Head back to the browser and tell the model your favorite pizza topping
    (note: pineapple is not an option).
  </Step>
  <Step title="Observe the Response">
    You should see a follow-up response from the model confirming the action and
    providing additional context.
  </Step>
  <Step title="Verify Tool Call Flow">
    The model will first call the tool, then provide a summary or confirmation
    message.
  </Step>
</Steps>

<Info>
  With multi-step processing, the AI can now provide a much better user
  experience by explaining what it did and why, rather than just executing the
  tool silently.
</Info>

---

## Alternative Stopping Conditions

You can configure different stopping conditions based on your needs:

<CodeGroup>

```typescript stopWhen-examples.ts
// Stop after a specific number of steps
stopWhen: stepCountIs(3);

// Stop when the model generates a specific pattern
stopWhen: textMatches(/^I have|^The tool/);

// Stop when the model reaches a certain token limit
stopWhen: tokenCountIs(100);

// Combine multiple conditions
stopWhen: [stepCountIs(5), textMatches(/^Summary:/)];
```

</CodeGroup>

---

## Benefits of Multi-Step Processing

<Columns cols={2}>
  <Card title="Better User Experience" icon="smile">
    **Clear Communication**: Users understand what actions were taken and why.
  </Card>
  <Card title="Error Recovery" icon="refresh">
    **Graceful Handling**: The model can handle tool failures and provide
    alternatives.
  </Card>
</Columns>
<Columns cols={2}>
  <Card title="Context Building" icon="layers">
    **Rich Responses**: The model can build upon tool results with additional
    insights.
  </Card>
  <Card title="Action Confirmation" icon="check-circle">
    **User Confidence**: Users receive confirmation that their requests were
    processed correctly.
  </Card>
</Columns>

---

## Testing Different Scenarios

<Steps>
  <Step title="Test Resource Addition">
    Try telling the model various pieces of information to see how it handles
    different types of content.
  </Step>
  <Step title="Observe Response Patterns">
    Notice how the model confirms actions and provides context after tool
    execution.
  </Step>
  <Step title="Check Database Updates">
    Verify that resources are being properly added to your knowledge base.
  </Step>
</Steps>

<Warning>
  Remember that the model will still respond "Sorry, I don't know" to questions
  it can't answer, as we haven't implemented the retrieval tool yet.
</Warning>

---

## Extension task

<AIPromptReflection
  question="How does multi-step processing improve the user experience compared to single-step tool calls? What are the trade-offs between allowing more steps vs. fewer steps?"
  chatgptButtonText="Ask ChatGPT"
  claudeButtonText="Ask Claude"
/>
```
