---
title: "4.2 API Routes & System Prompts"
description: "Set up API routes to handle chat requests and configure system prompts for RAG behavior."
---

import { AIPromptReflection } from "/snippets/ai-prompt-reflection.jsx";

## Creating the API Route

In Next.js, you can create custom request handlers for a given route using Route Handlers. Route Handlers are defined in a `route.ts` file and can export HTTP methods like GET, POST, PUT, PATCH, etc.

<Steps>
  <Step title="Create the File">
    Create a file at `app/api/chat/route.ts`.
  </Step>
  <Step title="Add the Code">
    Open the file and add the following code:

```typescript app/api/chat/route.ts
import { openai } from "@ai-sdk/openai";
import { convertToModelMessages, streamText, UIMessage } from "ai";

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

  </Step>
</Steps>

### How the API route works

<Steps>
  <Step title="Declare the POST Function">
    Export an asynchronous function called `POST` that handles incoming HTTP POST requests to this route.
  </Step>

<Step title="Extract Request Data">
  Retrieve the messages from the request body using `await request.json()` to
  get the conversation history.
</Step>

<Step title="Call AI SDK">
  Pass the messages to the `streamText` function imported from the AI SDK, along
  with the specified model configuration.
</Step>

  <Step title="Return Response">
    Return the model's streaming response in `UIMessageStreamResponse` format, which enables real-time text streaming to the client.
  </Step>
</Steps>

---

## Test the Basic Implementation

<Steps>
  <Step title="Start the Development Server">
    Run `pnpm run dev` and navigate to http://localhost:3000
  </Step>
  <Step title="Send a Test Message">
    Head back to the browser and try to send a message again. You should see a
    response from the model streamed directly in!
  </Step>
</Steps>

<Info>
  While you now have a working agent, it isn't doing anything special. We need
  to add system instructions to refine and restrict the model's behavior.
</Info>

---

## Adding System Prompts

Let's add system instructions to refine and restrict the model's behavior. In this case, you want the model to only use information it has retrieved to generate responses.

Update your route handler with the following code:

```typescript app/api/chat/route.ts highlight={12-14} lines
import { openai } from "@ai-sdk/openai";
import { convertToModelMessages, streamText, UIMessage } from "ai";

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    system: `You are a helpful assistant. Check your knowledge base before answering any questions.
    Only respond to questions using information from tool calls.
    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,
    messages: convertToModelMessages(messages),
  });

  return result.toUIMessageStreamResponse();
}
```

<Warning>
  In its current form, your agent is now, well, useless. The model will respond
  with "Sorry, I don't know" to any question because it doesn't have access to
  any knowledge base or tools yet.
</Warning>

---

## Test the System Prompt

<Steps>
  <Step title="Ask a Question">
    Head back to the browser and try to ask the model what your favorite food
    is. The model should now respond exactly as you instructed above ("Sorry, I
    don't know") given it doesn't have any relevant information.
  </Step>
  <Step title="Test Different Queries">
    Try various questions to verify the system prompt is working correctly.
  </Step>
</Steps>

<Info>
  The system prompt ensures the AI only responds using information from tool
  calls, which we haven't implemented yet. This prepares the foundation for RAG
  functionality.
</Info>

---

## Understanding the Implementation

<Columns cols={2}>
  <Card title="Request Handling" icon="inbox">
    **Message Parsing**: Extracts messages from the request body
  </Card>
  <Card title="Streaming Response" icon="zap">
    **Real-time Output**: Returns streaming response to the client
  </Card>
</Columns>
<Columns cols={2}>
  <Card title="Model Configuration" icon="bot">
    **OpenAI Integration**: Uses gpt-4o model for chat responses
  </Card>
  <Card title="System Prompt" icon="settings">
    **Behavior Control**: Restricts AI to only use tool-based information
  </Card>
</Columns>

---

## Extension tasks

<AIPromptReflection
  cardTitle="API Route options"
  question="What is the purpose of the `maxDuration` variable in the API route and why is it important?"
  chatgptButtonText="Ask ChatGPT"
  claudeButtonText="Ask Claude"
/>

<AIPromptReflection
  cardTitle="API Route application"
  question="Show me code examples of using API routes in a production application deployed on Vercel for processing payments and connecting to a database."
  chatgptButtonText="Ask ChatGPT"
  claudeButtonText="Ask Claude"
/>
