---
title: "7.1 API Route Setup"
description: "Create Next.js API routes for AI model integration, implement request processing, and set up streaming responses with proper error handling."
---

## Learning Objectives

By the end of this section, you will be able to:

- Create Next.js App Router API routes for AI model integration
- Implement request processing and validation
- Set up streaming responses with proper error handling
- Configure proper response headers and status codes

<Callout>**Duration**: 30 minutes</Callout>

---

## Next.js App Router API Routes

### Understanding App Router API Routes

<Callout type="info">
  **Route Handlers**: Next.js 13+ feature for creating API endpoints using the
  App Router
</Callout>

<Callout type="info">
  **File-based Routing**: API routes are created by adding `route.ts` files in
  the `app/api` directory
</Callout>

<Callout type="info">
  **Streaming Support**: Built-in support for streaming responses and real-time
  data
</Callout>

### Basic API Route Structure

<CodeGroup>
  <CodeGroupItem title="Basic Chat API Route">
```typescript
// app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server'

export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    const { messages } = body

    // Validate request
    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json(
        { error: 'Messages array is required' },
        { status: 400 }
      )
    }

    // Process the request
    const response = await processChatRequest(messages)

    return NextResponse.json(response)

} catch (error) {
console.error('Chat API error:', error)
return NextResponse.json(
{ error: 'Internal server error' },
{ status: 500 }
)
}
}

async function processChatRequest(messages: any[]) {
// TODO: Implement AI model integration
return {
message: 'Hello from the API!',
timestamp: new Date().toISOString()
}
}

````
  </CodeGroupItem>
  <CodeGroupItem title="Enhanced with Validation">
```typescript
// app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { z } from 'zod'

// Request validation schema
const chatRequestSchema = z.object({
  messages: z.array(z.object({
    id: z.string().optional(),
    role: z.enum(['user', 'assistant', 'system']),
    content: z.string().min(1, 'Message content is required'),
    createdAt: z.date().optional()
  })).min(1, 'At least one message is required'),
  model: z.string().optional().default('gpt-4o'),
  temperature: z.number().min(0).max(2).optional().default(0.7),
  maxTokens: z.number().min(1).max(4000).optional().default(1000)
})

export async function POST(request: NextRequest) {
  try {
    // Parse and validate request body
    const body = await request.json()
    const validatedData = chatRequestSchema.parse(body)

    // Check rate limiting
    const clientIP = request.ip || request.headers.get('x-forwarded-for')
    if (await isRateLimited(clientIP)) {
      return NextResponse.json(
        { error: 'Rate limit exceeded' },
        { status: 429 }
      )
    }

    // Process the chat request
    const response = await processChatRequest(validatedData)

    return NextResponse.json(response)
  } catch (error) {
    if (error instanceof z.ZodError) {
      return NextResponse.json(
        {
          error: 'Validation failed',
          details: error.errors
        },
        { status: 400 }
      )
    }

    console.error('Chat API error:', error)
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    )
  }
}

async function isRateLimited(clientIP: string | null): Promise<boolean> {
  // TODO: Implement rate limiting logic
  return false
}

async function processChatRequest(data: z.infer<typeof chatRequestSchema>) {
  // TODO: Implement AI model integration
  return {
    message: 'Hello from the validated API!',
    timestamp: new Date().toISOString()
  }
}
````

  </CodeGroupItem>
</CodeGroup>

---

## Streaming Response Implementation

### AI SDK Streaming Integration

<CodeGroup>
  <CodeGroupItem title="Streaming Chat API">
```typescript
// app/api/chat/route.ts
import { NextRequest } from 'next/server'
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'
import { z } from 'zod'

const chatRequestSchema = z.object({
messages: z.array(z.object({
role: z.enum(['user', 'assistant', 'system']),
content: z.string()
})),
model: z.string().optional().default('gpt-4o'),
temperature: z.number().optional().default(0.7),
maxTokens: z.number().optional().default(1000)
})

export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    const { messages, model, temperature, maxTokens } = chatRequestSchema.parse(body)

    // Create streaming response
    const result = await streamText({
      model: openai(model),
      messages,
      temperature,
      maxTokens,
      system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
    })

    return result.toDataStreamResponse()

} catch (error) {
console.error('Streaming chat error:', error)

    if (error instanceof z.ZodError) {
      return new Response(
        JSON.stringify({ error: 'Validation failed', details: error.errors }),
        {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        }
      )
    }

    return new Response(
      JSON.stringify({ error: 'Internal server error' }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    )

}
}

````
  </CodeGroupItem>
  <CodeGroupItem title="Enhanced Streaming with Error Handling">
```typescript
// app/api/chat/route.ts
import { NextRequest } from 'next/server'
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'
import { z } from 'zod'

const chatRequestSchema = z.object({
  messages: z.array(z.object({
    role: z.enum(['user', 'assistant', 'system']),
    content: z.string().min(1, 'Message content cannot be empty')
  })).min(1, 'At least one message is required'),
  model: z.string().optional().default('gpt-4o'),
  temperature: z.number().min(0).max(2).optional().default(0.7),
  maxTokens: z.number().min(1).max(4000).optional().default(1000),
  stream: z.boolean().optional().default(true)
})

export async function POST(request: NextRequest) {
  try {
    // Parse and validate request
    const body = await request.json()
    const { messages, model, temperature, maxTokens, stream } = chatRequestSchema.parse(body)

    // Check authentication (if needed)
    const authHeader = request.headers.get('authorization')
    if (!authHeader) {
      return new Response(
        JSON.stringify({ error: 'Authorization required' }),
        { status: 401, headers: { 'Content-Type': 'application/json' } }
      )
    }

    // Rate limiting check
    const clientIP = request.ip || request.headers.get('x-forwarded-for')
    if (await isRateLimited(clientIP)) {
      return new Response(
        JSON.stringify({ error: 'Rate limit exceeded. Please try again later.' }),
        { status: 429, headers: { 'Content-Type': 'application/json' } }
      )
    }

    if (stream) {
      // Streaming response
      const result = await streamText({
        model: openai(model),
        messages,
        temperature,
        maxTokens,
        system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`,
        onError: (error) => {
          console.error('Streaming error:', error)
        }
      })

      return result.toDataStreamResponse()
    } else {
      // Non-streaming response
      const result = await openai(model).generate({
        messages,
        temperature,
        maxTokens,
        system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
      })

      return new Response(
        JSON.stringify({
          message: result.text,
          usage: result.usage
        }),
        {
          status: 200,
          headers: { 'Content-Type': 'application/json' }
        }
      )
    }
  } catch (error) {
    console.error('Chat API error:', error)

    if (error instanceof z.ZodError) {
      return new Response(
        JSON.stringify({
          error: 'Validation failed',
          details: error.errors
        }),
        {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        }
      )
    }

    // Handle OpenAI API errors
    if (error.message?.includes('rate_limit')) {
      return new Response(
        JSON.stringify({ error: 'Rate limit exceeded. Please try again later.' }),
        { status: 429, headers: { 'Content-Type': 'application/json' } }
      )
    }

    if (error.message?.includes('insufficient_quota')) {
      return new Response(
        JSON.stringify({ error: 'Service temporarily unavailable.' }),
        { status: 503, headers: { 'Content-Type': 'application/json' } }
      )
    }

    return new Response(
      JSON.stringify({ error: 'Internal server error' }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    )
  }
}

async function isRateLimited(clientIP: string | null): Promise<boolean> {
  // TODO: Implement rate limiting logic
  // This could use Redis, database, or in-memory storage
  return false
}
````

  </CodeGroupItem>
</CodeGroup>

---

## Request Processing and Validation

### Comprehensive Request Handling

<CodeGroup>
  <CodeGroupItem title="Request Middleware">
```typescript
// lib/api-middleware.ts
import { NextRequest } from 'next/server'
import { z } from 'zod'

export interface ApiContext {
  request: NextRequest
  clientIP: string | null
  userAgent: string | null
  startTime: number
}

export async function createApiContext(request: NextRequest): Promise<ApiContext> {
  return {
    request,
    clientIP: request.ip || request.headers.get('x-forwarded-for'),
    userAgent: request.headers.get('user-agent'),
    startTime: Date.now()
  }
}

export function validateRequest<T>(schema: z.ZodSchema<T>, body: any): T {
  return schema.parse(body)
}

export function createErrorResponse(error: any, status: number = 500) {
  console.error('API Error:', error)
  
  const errorMessage = error instanceof z.ZodError 
    ? 'Validation failed'
    : error.message || 'Internal server error'
  
  const errorDetails = error instanceof z.ZodError 
    ? error.errors 
    : undefined

return new Response(
JSON.stringify({
error: errorMessage,
details: errorDetails,
timestamp: new Date().toISOString()
}),
{
status,
headers: { 'Content-Type': 'application/json' }
}
)
}

export function logApiRequest(context: ApiContext, status: number) {
  const duration = Date.now() - context.startTime
  console.log(`API Request: ${context.request.method} ${context.request.url} - ${status} (${duration}ms)`)
}
```
  </CodeGroupItem>
  <CodeGroupItem title="Enhanced API Route with Middleware">
```typescript
// app/api/chat/route.ts
import { NextRequest } from 'next/server'
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'
import { z } from 'zod'
import { 
  createApiContext, 
  validateRequest, 
  createErrorResponse, 
  logApiRequest 
} from '@/lib/api-middleware'

const chatRequestSchema = z.object({
messages: z.array(z.object({
role: z.enum(['user', 'assistant', 'system']),
content: z.string().min(1, 'Message content cannot be empty')
})).min(1, 'At least one message is required'),
model: z.string().optional().default('gpt-4o'),
temperature: z.number().min(0).max(2).optional().default(0.7),
maxTokens: z.number().min(1).max(4000).optional().default(1000),
stream: z.boolean().optional().default(true)
})

export async function POST(request: NextRequest) {
  const context = await createApiContext(request)
  
  try {
    // Validate request method
    if (request.method !== 'POST') {
      return createErrorResponse(
        new Error('Method not allowed'),
        405
      )
    }

    // Parse and validate request body
    const body = await request.json()
    const validatedData = validateRequest(chatRequestSchema, body)

    // Check authentication
    const authHeader = request.headers.get('authorization')
    if (!authHeader) {
      return createErrorResponse(
        new Error('Authorization required'),
        401
      )
    }

    // Rate limiting
    if (await isRateLimited(context.clientIP)) {
      return createErrorResponse(
        new Error('Rate limit exceeded'),
        429
      )
    }

    // Process request
    const { messages, model, temperature, maxTokens, stream } = validatedData

    if (stream) {
      const result = await streamText({
        model: openai(model),
        messages,
        temperature,
        maxTokens,
        system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
      })

      logApiRequest(context, 200)
      return result.toDataStreamResponse()
    } else {
      const result = await openai(model).generate({
        messages,
        temperature,
        maxTokens,
        system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
      })

      logApiRequest(context, 200)
      return new Response(
        JSON.stringify({
          message: result.text,
          usage: result.usage
        }),
        {
          status: 200,
          headers: { 'Content-Type': 'application/json' }
        }
      )
    }

} catch (error) {
const status = error instanceof z.ZodError ? 400 : 500
logApiRequest(context, status)
return createErrorResponse(error, status)
}
}

async function isRateLimited(clientIP: string | null): Promise<boolean> {
// TODO: Implement rate limiting
return false
}

````
  </CodeGroupItem>
</CodeGroup>

---

## Error Handling and Response Management

### Comprehensive Error Handling

<CodeGroup>
  <CodeGroupItem title="Error Types and Handling">
```typescript
// lib/api-errors.ts
export class ApiError extends Error {
  constructor(
    message: string,
    public statusCode: number = 500,
    public code?: string
  ) {
    super(message)
    this.name = 'ApiError'
  }
}

export class ValidationError extends ApiError {
  constructor(message: string, public details?: any[]) {
    super(message, 400, 'VALIDATION_ERROR')
    this.name = 'ValidationError'
    this.details = details
  }
}

export class AuthenticationError extends ApiError {
  constructor(message: string = 'Authentication required') {
    super(message, 401, 'AUTHENTICATION_ERROR')
    this.name = 'AuthenticationError'
  }
}

export class RateLimitError extends ApiError {
  constructor(message: string = 'Rate limit exceeded') {
    super(message, 429, 'RATE_LIMIT_ERROR')
    this.name = 'RateLimitError'
  }
}

export class ModelError extends ApiError {
  constructor(message: string, public modelError?: any) {
    super(message, 500, 'MODEL_ERROR')
    this.name = 'ModelError'
    this.modelError = modelError
  }
}

export function handleApiError(error: any): Response {
  if (error instanceof ApiError) {
    return new Response(
      JSON.stringify({
        error: error.message,
        code: error.code,
        details: error.details,
        timestamp: new Date().toISOString()
      }),
      {
        status: error.statusCode,
        headers: { 'Content-Type': 'application/json' }
      }
    )
  }

  // Handle OpenAI API errors
  if (error.message?.includes('rate_limit')) {
    return new Response(
      JSON.stringify({
        error: 'Rate limit exceeded. Please try again later.',
        code: 'RATE_LIMIT_ERROR',
        timestamp: new Date().toISOString()
      }),
      { status: 429, headers: { 'Content-Type': 'application/json' } }
    )
  }

  if (error.message?.includes('insufficient_quota')) {
    return new Response(
      JSON.stringify({
        error: 'Service temporarily unavailable.',
        code: 'QUOTA_EXCEEDED',
        timestamp: new Date().toISOString()
      }),
      { status: 503, headers: { 'Content-Type': 'application/json' } }
    )
  }

  // Generic error
  console.error('Unhandled API error:', error)
  return new Response(
    JSON.stringify({
      error: 'Internal server error',
      code: 'INTERNAL_ERROR',
      timestamp: new Date().toISOString()
    }),
    { status: 500, headers: { 'Content-Type': 'application/json' } }
  )
}
````

  </CodeGroupItem>
  <CodeGroupItem title="Enhanced API Route with Error Handling">
```typescript
// app/api/chat/route.ts
import { NextRequest } from 'next/server'
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'
import { z } from 'zod'
import { 
  ValidationError, 
  AuthenticationError, 
  RateLimitError, 
  ModelError,
  handleApiError 
} from '@/lib/api-errors'

const chatRequestSchema = z.object({
messages: z.array(z.object({
role: z.enum(['user', 'assistant', 'system']),
content: z.string().min(1, 'Message content cannot be empty')
})).min(1, 'At least one message is required'),
model: z.string().optional().default('gpt-4o'),
temperature: z.number().min(0).max(2).optional().default(0.7),
maxTokens: z.number().min(1).max(4000).optional().default(1000),
stream: z.boolean().optional().default(true)
})

export async function POST(request: NextRequest) {
  try {
    // Validate request method
    if (request.method !== 'POST') {
      throw new ValidationError('Method not allowed')
    }

    // Parse and validate request body
    let body
    try {
      body = await request.json()
    } catch {
      throw new ValidationError('Invalid JSON in request body')
    }

    const validatedData = chatRequestSchema.parse(body)

    // Check authentication
    const authHeader = request.headers.get('authorization')
    if (!authHeader) {
      throw new AuthenticationError()
    }

    // Rate limiting
    const clientIP = request.ip || request.headers.get('x-forwarded-for')
    if (await isRateLimited(clientIP)) {
      throw new RateLimitError()
    }

    // Process request
    const { messages, model, temperature, maxTokens, stream } = validatedData

    try {
      if (stream) {
        const result = await streamText({
          model: openai(model),
          messages,
          temperature,
          maxTokens,
          system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
        })

        return result.toDataStreamResponse()
      } else {
        const result = await openai(model).generate({
          messages,
          temperature,
          maxTokens,
          system: `You are a helpful RAG assistant. Answer questions based on the provided context and be concise but informative.`
        })

        return new Response(
          JSON.stringify({
            message: result.text,
            usage: result.usage
          }),
          {
            status: 200,
            headers: { 'Content-Type': 'application/json' }
          }
        )
      }
    } catch (modelError) {
      throw new ModelError('Failed to generate response', modelError)
    }

} catch (error) {
return handleApiError(error)
}
}

async function isRateLimited(clientIP: string | null): Promise<boolean> {
// TODO: Implement rate limiting
return false
}

```
  </CodeGroupItem>
</CodeGroup>

---

## Interactive Elements

### API Route Testing Checklist

<Callout type="info">
  **Complete this checklist to verify your API route setup**:

  ✅ API route responds to POST requests
  ✅ Request validation works correctly
  ✅ Streaming responses function properly
  ✅ Error handling covers all scenarios
  ✅ Rate limiting is implemented
  ✅ Authentication is enforced
  ✅ Response headers are set correctly
  ✅ Logging and monitoring work
  ✅ Performance is acceptable
  ✅ Security measures are in place
</Callout>

### API Testing Scenarios

<Callout type="info">
  **Test these API scenarios**:

  1. Valid chat request with streaming
  2. Valid chat request without streaming
  3. Invalid request (missing messages)
  4. Invalid request (wrong message format)
  5. Rate limited request
  6. Unauthorized request
  7. Model error handling
  8. Network timeout handling
  9. Large message handling
  10. Concurrent request handling
</Callout>

---

## Troubleshooting Common Issues

<Callout type="warning">
  **Issue**: API route not responding
  **Solution**: Check file location, export function names, and Next.js configuration
</Callout>

<Callout type="warning">
  **Issue**: Streaming not working
  **Solution**: Verify AI SDK setup, check response headers, and ensure proper streaming implementation
</Callout>

<Callout type="warning">
  **Issue**: Validation errors not handled
  **Solution**: Implement proper error handling middleware and Zod validation
</Callout>

<Callout type="warning">
  **Issue**: Rate limiting not working
  **Solution**: Check rate limiting implementation and storage mechanism
</Callout>

---

## Reflection Questions

Take a moment to reflect on what you've learned:

1. **How do API routes enable AI model integration in Next.js applications?**
   - Consider server-side processing and client communication
   - Think about streaming and real-time responses

2. **What are the key considerations for building robust API endpoints?**
   - Consider error handling, validation, and security
   - Think about performance and scalability

3. **How would you optimize the API route for your specific use case?**
   - Consider caching, rate limiting, and monitoring
   - Think about security and authentication requirements

---

## Next Steps

You've set up the API routes! In the next section, you'll:

- **Configure model parameters** and system prompts
- **Implement RAG-specific functionality** for context retrieval
- **Add advanced features** like conversation memory

Ready to continue? Proceed to [Section 7.2: Model Configuration](/learning-paths/rag-chatbot/module-7-2).

<Callout>
  **Key Takeaway**: Proper API route setup with comprehensive error handling and streaming support provides the foundation for reliable AI model integration in your RAG application.
</Callout>
```
